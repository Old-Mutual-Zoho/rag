# RAG pipeline configuration (embeddings, vector store, retrieval, generation)
# Vector store provider: pgvector (Neon/Postgres) or qdrant_* (local/remote Qdrant)

embeddings:
  provider: gemini  # sentence_transformers | openai | gemini | ollama
  model: models/text-embedding-004
  # For OpenAI: model: text-embedding-3-small
  # For Gemini: model: models/text-embedding-004, api_key_env: GEMINI_API_KEY
  # For Ollama: model: nomic-embed-text, base_url: http://localhost:11434

vector_store:
  provider: pgvector   # pgvector | qdrant_local | qdrant_http
  collection: old_mutual_chunks
  # pgvector: uses DATABASE_URL from environment
  # qdrant_local: path: data/qdrant
  # qdrant_http: host: localhost, port: 6333

retrieval:
  top_k: 10   # fetch more chunks so product-specific queries (e.g. Somesa Plan) get relevant context
  hybrid:
    enabled: false
    dense_weight: 0.7
    sparse_weight: 0.3

generation:
  enabled: true
  backend: gemini
  # If you see "free_tier ... limit: 0", enable billing in Google AI Studio for this project
  model: "models/gemini-flash-latest"
  api_key_env: GEMINI_API_KEY
